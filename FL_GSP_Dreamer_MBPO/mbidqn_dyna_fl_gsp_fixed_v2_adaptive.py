"""
mbidqn_dyna_fl_gsp_fixed_v2_adaptive.py - å¸¦åŠ¨æ€æ§åˆ¶çš„Dynaç‰ˆæœ¬

æ ¸å¿ƒæ”¹è¿›ï¼š
1. âœ… Real dataä¿åº•æœºåˆ¶ï¼šmin_real_ratioç¡®ä¿real dataæœ€å°æ¯”ä¾‹
2. âœ… Synthetic dataä½œä¸ºå¢é‡ï¼šæ ¹æ®WMè´¨é‡åŠ¨æ€è°ƒæ•´
3. âœ… åŠ¨æ€horizonè°ƒæ•´ï¼šä»å°horizoné€æ­¥å¢åŠ åˆ°max_horizon
4. âœ… è´¨é‡ç›‘æ§ï¼šè¿½è¸ªWMè´¨é‡å¹¶è‡ªé€‚åº”è°ƒæ•´ç­–ç•¥

å…³é”®å…¬å¼ï¼š
- real_ratio = max(min_real_ratio, 1.0 - wm_quality * adaptive_factor)
- horizon = min(current_horizon, max_horizon) éšepisodeå¢é•¿

ä½œè€…: Percy Zhang
æ—¥æœŸ: 2025-11-28
ç‰ˆæœ¬: Adaptive V2
"""

import os
import logging
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import deque
from copy import deepcopy
import pfrl
from pfrl.explorers import LinearDecayEpsilonGreedy
from pfrl.q_functions import DiscreteActionValueHead

from resco_benchmark.config.config import config as cfg
from resco_benchmark.agents.agent import IndependentAgent, Agent
from resco_benchmark.utils.utils import compute_safe_id

logger = logging.getLogger(__name__)


# ==================== Adaptive Controller ====================

class AdaptiveDataController:
    """
    è‡ªé€‚åº”æ•°æ®æ§åˆ¶å™¨ - Synthetic dataä½œä¸ºReal dataçš„å¢é‡
    
    æ ¸å¿ƒç­–ç•¥ï¼š
    1. Real dataå›ºå®šï¼šå§‹ç»ˆä½¿ç”¨å›ºå®šbatch sizeçš„real dataï¼ˆå¦‚32ï¼‰
    2. Synthetic dataå¢é‡ï¼šæ ¹æ®WMè´¨é‡åŠ¨æ€å¢åŠ synthetic batch size
    3. Horizonå¢é•¿ï¼šæ¯Nä¸ªepisodeså¢åŠ 1æ­¥ï¼Œç›´åˆ°max_horizon
    
    å…³é”®ï¼šReal dataä¸ä¼šè¢«ç¨€é‡Šï¼Œsyntheticæ˜¯é¢å¤–çš„bonus
    """
    def __init__(self, 
                 base_real_batch=32,           # Real dataå›ºå®šbatch size
                 max_synthetic_batch=64,       # Synthetic dataæœ€å¤§batch size
                 initial_horizon=1,            # åˆå§‹horizon
                 max_horizon=5,                # æœ€å¤§horizon
                 horizon_increase_freq=10,     # æ¯Nä¸ªepisodeså¢åŠ horizon
                 quality_threshold=0.3,        # WMè´¨é‡é˜ˆå€¼ï¼ˆä½äºæ­¤ä¸ç”¨syntheticï¼‰
                 adaptive_factor=1.0):         # è‡ªé€‚åº”è°ƒæ•´å› å­
        
        self.base_real_batch = base_real_batch
        self.max_synthetic_batch = max_synthetic_batch
        self.initial_horizon = initial_horizon
        self.max_horizon = max_horizon
        self.horizon_increase_freq = horizon_increase_freq
        self.quality_threshold = quality_threshold
        self.adaptive_factor = adaptive_factor
        
        # å½“å‰çŠ¶æ€
        self.current_horizon = initial_horizon
        self.current_synthetic_batch = 0  # åˆå§‹0 synthetic
        self.episode_count = 0
        
        # è´¨é‡å†å²
        self.quality_history = deque(maxlen=100)
        
        logger.info(f"AdaptiveDataController initialized:")
        logger.info(f"  base_real_batch={base_real_batch} (å›ºå®š)")
        logger.info(f"  max_synthetic_batch={max_synthetic_batch}")
        logger.info(f"  horizon: {initial_horizon} â†’ {max_horizon}")
        logger.info(f"  horizon_increase_freq={horizon_increase_freq}")
    
    def update_quality(self, wm_quality):
        """æ›´æ–°WMè´¨é‡"""
        self.quality_history.append(wm_quality)
    
    def get_current_synthetic_batch(self):
        """
        è®¡ç®—å½“å‰synthetic data batch sizeï¼ˆä½œä¸ºå¢é‡ï¼‰
        
        ç­–ç•¥ï¼š
        - WMè´¨é‡ä½ï¼ˆ< thresholdï¼‰â†’ synthetic = 0ï¼ˆä¸ç”¨ï¼‰
        - WMè´¨é‡ä¸­ç­‰ â†’ syntheticé€æ­¥å¢åŠ 
        - WMè´¨é‡é«˜ â†’ syntheticè¾¾åˆ°max_synthetic_batch
        
        é‡è¦ï¼šè¿”å›çš„æ˜¯synthetic batch sizeï¼Œä¸å½±å“real batch
        """
        if len(self.quality_history) == 0:
            return 0  # æ²¡æœ‰è´¨é‡æ•°æ®æ—¶ï¼Œä¸ç”¨synthetic
        
        # ä½¿ç”¨æœ€è¿‘çš„å¹³å‡è´¨é‡
        avg_quality = np.mean(list(self.quality_history))
        
        # å¦‚æœè´¨é‡ä½äºé˜ˆå€¼ï¼Œä¸ç”¨synthetic
        if avg_quality < self.quality_threshold:
            target_synthetic_batch = 0
        else:
            # è´¨é‡å¥½æ—¶ï¼Œæ ¹æ®è´¨é‡å¢åŠ synthetic
            # qualityè¶Šé«˜ï¼Œsyntheticè¶Šå¤š
            # ä¾‹å¦‚ï¼šquality=0.5 â†’ synthetic=32, quality=1.0 â†’ synthetic=64
            quality_above_threshold = avg_quality - self.quality_threshold
            max_quality_range = 1.0 - self.quality_threshold
            
            normalized_quality = quality_above_threshold / max_quality_range
            target_synthetic_batch = int(
                normalized_quality * self.adaptive_factor * self.max_synthetic_batch
            )
            target_synthetic_batch = min(target_synthetic_batch, self.max_synthetic_batch)
        
        # å¹³æ»‘è¿‡æ¸¡
        self.current_synthetic_batch = int(
            0.9 * self.current_synthetic_batch + 0.1 * target_synthetic_batch
        )
        
        return self.current_synthetic_batch
    
    def get_current_horizon(self):
        """è·å–å½“å‰imagination horizon"""
        return self.current_horizon
    
    def on_episode_end(self):
        """Episodeç»“æŸæ—¶è°ƒç”¨ - æ›´æ–°horizon"""
        self.episode_count += 1
        
        # æ¯Nä¸ªepisodeså¢åŠ horizon
        if self.episode_count % self.horizon_increase_freq == 0:
            if self.current_horizon < self.max_horizon:
                self.current_horizon += 1
                logger.info(f"ğŸ“ˆ Horizon increased to {self.current_horizon} "
                          f"(episode {self.episode_count})")
    
    def should_use_imagination(self, step_count, warmup_steps):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨imagination"""
        if step_count < warmup_steps:
            return False
        
        # æ£€æŸ¥WMè´¨é‡
        if len(self.quality_history) > 0:
            avg_quality = np.mean(list(self.quality_history))
            if avg_quality < 0.2:  # è´¨é‡å¤ªå·®ï¼Œä¸ç”¨imagination
                return False
        
        return True
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if len(self.quality_history) > 0:
            avg_quality = np.mean(list(self.quality_history))
        else:
            avg_quality = 0.0
        
        total_batch = self.base_real_batch + self.current_synthetic_batch
        
        return {
            'episode': self.episode_count,
            'horizon': self.current_horizon,
            'real_batch': self.base_real_batch,
            'synthetic_batch': self.current_synthetic_batch,
            'total_batch': total_batch,
            'avg_wm_quality': avg_quality
        }


# ==================== Copy the RSSM and other classes from original v2 ====================
# [è¿™é‡Œéœ€è¦å¤åˆ¶åŸå§‹v2æ–‡ä»¶ä¸­çš„æ‰€æœ‰ç±»å®šä¹‰]
# ä¸ºäº†ç®€æ´ï¼Œæˆ‘åªå±•ç¤ºå…³é”®ä¿®æ”¹éƒ¨åˆ†

# ... [å¤åˆ¶ GlobalCoordinatorWithGSP, RSSM_with_GSP, ReplayBuffer ç­‰ç±»] ...


class DynaMBAgent_Adaptive:
    """
    Dyna-style MB Agent with Adaptive Control
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. ä½¿ç”¨AdaptiveDataControlleråŠ¨æ€è°ƒæ•´ç­–ç•¥
    2. æ··åˆé‡‡æ ·ç¡®ä¿real dataä¿åº•
    3. åŠ¨æ€horizonå¢é•¿
    """
    def __init__(self, agent_id, obs_space, act_space, coordinator=None):
        self.agent_id = agent_id
        self.obs_space = obs_space
        self.act_space = act_space
        self.coordinator = coordinator
        
        # Device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Dimensions
        if len(obs_space) == 1:
            obs_dim = obs_space[0]
        else:
            obs_dim = int(np.prod(obs_space))
        self.obs_dim = obs_dim
        self.act_dim = act_space
        
        # â­ åˆ›å»ºè‡ªé€‚åº”æ§åˆ¶å™¨
        self.adaptive_controller = AdaptiveDataController(
            base_real_batch=cfg.batch_size,  # Real batchå›ºå®šä¸º32
            max_synthetic_batch=cfg.get('max_synthetic_batch', 64),
            initial_horizon=cfg.get('initial_horizon', 1),
            max_horizon=cfg.get('imagination_horizon', 5),
            horizon_increase_freq=cfg.get('horizon_increase_freq', 10),
            quality_threshold=cfg.get('min_wm_quality', 0.3),
            adaptive_factor=cfg.get('adaptive_factor', 1.0)
        )
        
        # World Model
        self.world_model = RSSM_with_GSP(
            obs_dim=obs_dim,
            action_dim=act_space,
            hidden_dim=cfg.number_of_units,
            stoch_dim=32,
            deter_dim=cfg.number_of_units,
            global_dim=cfg.get('global_dim', 64)
        ).to(self.device)
        
        # Q-Network
        hidden_dim = cfg.number_of_units
        feature_dim = self.world_model.get_feature_size()
        self.q_network = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            DiscreteActionValueHead(hidden_dim, act_space)
        ).to(self.device)
        
        self.target_q_network = deepcopy(self.q_network)
        
        # Optimizers
        self.wm_optimizer = torch.optim.Adam(
            self.world_model.parameters(), 
            lr=cfg.get('lr_world_model', 5e-4)
        )
        self.q_optimizer = torch.optim.RMSprop(
            self.q_network.parameters(),
            lr=cfg.learning_rate,
            alpha=cfg.get('rmsprop_decay', 0.95),
            eps=cfg.get('rmsprop_epsilon', 1e-8),
            momentum=cfg.get('rmsprop_momentum', 0.0)
        )
        
        # Replay Buffers
        self.replay_buffer = ReplayBuffer(cfg.buffer_size)          # Real data
        self.imagined_buffer = ReplayBuffer(cfg.buffer_size // 2)   # Imagined data
        
        # Counters
        self.step_count = 0
        self.episode_count = 0
        self.global_step = 0
        
        # Config
        self.model_warmup_steps = cfg.get('model_warmup_steps', 1000)
        self.imagination_freq = cfg.get('imagination_freq', 5)
        self.num_imagined_rollouts = cfg.get('num_imagined_rollouts', 1)
        
        # FL
        self.fl_interval = cfg.get('fl_interval', 100)
        self.global_params = None
        
        logger.info(f"DynaMBAgent_Adaptive created for {agent_id}")
        logger.info(f"  Adaptive control enabled")
        logger.info(f"  Initial horizon: {self.adaptive_controller.initial_horizon}")
        logger.info(f"  Max horizon: {self.adaptive_controller.max_horizon}")
    
    def act(self, obs):
        """é€‰æ‹©åŠ¨ä½œ"""
        obs_t = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            state = self._encode_obs(obs_t)
            feat = self.world_model.get_feature(state)
            q_values = self.q_network(feat).q_values
            action = q_values.argmax(dim=1).item()
        
        return action
    
    def observe(self, obs, action, reward, next_obs, done):
        """è§‚å¯Ÿä¸€ä¸ªtransition"""
        # å­˜å‚¨åˆ°real buffer
        self.replay_buffer.push(obs, action, reward, next_obs, done)
        self.step_count += 1
        self.global_step += 1
        
        # è®­ç»ƒWorld Model
        if len(self.replay_buffer) >= cfg.batch_size:
            self._train_world_model()
            
            # â­ æ›´æ–°WMè´¨é‡åˆ°adaptive controller
            wm_quality = self.get_quality_score()
            self.adaptive_controller.update_quality(wm_quality)
        
        # â­ æ ¹æ®adaptive controllerå†³å®šæ˜¯å¦ç”Ÿæˆimagined data
        if self._should_generate_imagined_data():
            self._generate_imagined_data()
        
        # â­ ä½¿ç”¨adaptiveæ··åˆé‡‡æ ·è®­ç»ƒQ
        if len(self.replay_buffer) >= cfg.batch_size:
            self._train_q_adaptive()
        
        # ä¸Šä¼ FLå‚æ•°
        if self.step_count % self.fl_interval == 0:
            self._upload_fl_params()
        
        # Upload GSP
        if self.coordinator:
            with torch.no_grad():
                obs_t = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
                state = self._encode_obs(obs_t)
                G_hat = self.world_model.predict_global(state)
                self.coordinator.receive_gsp_prediction(
                    self.global_step, self.agent_id, G_hat
                )
    
    def _should_generate_imagined_data(self):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”Ÿæˆimagined data"""
        # Warmupé˜¶æ®µä¸ç”Ÿæˆ
        if not self.adaptive_controller.should_use_imagination(
            self.step_count, self.model_warmup_steps
        ):
            return False
        
        # æŒ‰é¢‘ç‡ç”Ÿæˆ
        if self.step_count % self.imagination_freq != 0:
            return False
        
        return True
    
    def _generate_imagined_data(self):
        """â­ ç”Ÿæˆimagined data - ä½¿ç”¨åŠ¨æ€horizon"""
        if len(self.replay_buffer) < cfg.batch_size:
            return
        
        # â­ è·å–å½“å‰çš„åŠ¨æ€horizon
        horizon = self.adaptive_controller.get_current_horizon()
        
        # ä»real bufferé‡‡æ ·èµ·å§‹çŠ¶æ€
        transitions = self.replay_buffer.sample_transitions(
            min(self.num_imagined_rollouts, len(self.replay_buffer))
        )
        
        with torch.no_grad():
            for trans in transitions:
                obs = torch.FloatTensor(trans['obs']).unsqueeze(0).to(self.device)
                state = self._encode_obs(obs)
                
                # â­ Rolloutä½¿ç”¨åŠ¨æ€horizon
                for step in range(horizon):
                    # ç”¨å½“å‰policyé€‰action
                    feat = self.world_model.get_feature(state)
                    q_values = self.q_network(feat).q_values
                    action = q_values.argmax(dim=1).item()
                    
                    # Imagine next state
                    action_onehot = torch.zeros(1, self.act_dim).to(self.device)
                    action_onehot[0, action] = 1.0
                    next_state = self.world_model.imagine_step(state, action_onehot)
                    
                    # Decode reward
                    _, reward = self.world_model.decode(next_state)
                    reward = reward.squeeze(-1).item()
                    
                    # å­˜å‚¨åˆ°imagined buffer
                    curr_obs = self.world_model.decode(state)[0].squeeze(0).cpu().numpy()
                    next_obs = self.world_model.decode(next_state)[0].squeeze(0).cpu().numpy()
                    
                    self.imagined_buffer.push(
                        curr_obs, action, reward, next_obs, False
                    )
                    
                    state = next_state
    
    def _train_q_adaptive(self):
        """â­ ä½¿ç”¨adaptiveæ§åˆ¶è®­ç»ƒQ-Network - Syntheticä½œä¸ºå¢é‡"""
        # â­ Real batchæ˜¯å›ºå®šçš„ï¼ˆå¦‚32ï¼‰
        real_batch_size = self.adaptive_controller.base_real_batch
        
        # â­ Synthetic batchæ˜¯åŠ¨æ€çš„ï¼ˆ0åˆ°max_synthetic_batchï¼‰
        synthetic_batch_size = self.adaptive_controller.get_current_synthetic_batch()
        
        # é‡‡æ ·real dataï¼ˆå›ºå®šæ•°é‡ï¼‰
        real_transitions = self.replay_buffer.sample_transitions(real_batch_size)
        
        # é‡‡æ ·synthetic dataï¼ˆåŠ¨æ€æ•°é‡ï¼Œå¯èƒ½æ˜¯0ï¼‰
        if len(self.imagined_buffer) > 0 and synthetic_batch_size > 0:
            imagined_transitions = self.imagined_buffer.sample_transitions(
                min(synthetic_batch_size, len(self.imagined_buffer))
            )
            # åˆå¹¶
            transitions = real_transitions + imagined_transitions
        else:
            # å¦‚æœæ²¡æœ‰synthetic dataæˆ–synthetic_batch_size=0ï¼Œåªç”¨real
            transitions = real_transitions
        
        # å‡†å¤‡batch
        obs = np.stack([t['obs'] for t in transitions])
        actions = np.array([t['action'] for t in transitions])
        rewards = np.array([t['reward'] for t in transitions])
        next_obs = np.stack([t['next_obs'] for t in transitions])
        dones = np.array([t['done'] for t in transitions])
        
        batch_size = len(transitions)  # åŠ¨æ€æ€»batch size
        
        obs_t = torch.FloatTensor(obs).to(self.device)
        actions_t = torch.LongTensor(actions).to(self.device)
        rewards_t = torch.FloatTensor(rewards).to(self.device)
        next_obs_t = torch.FloatTensor(next_obs).to(self.device)
        dones_t = torch.FloatTensor(dones).to(self.device)
        
        # Q-learning update
        with torch.no_grad():
            next_state = self._encode_obs(next_obs_t)
            next_feat = self.world_model.get_feature(next_state)
            next_q = self.target_q_network(next_feat).q_values.max(dim=1)[0]
            target = rewards_t + cfg.discount * (1 - dones_t) * next_q
        
        state = self._encode_obs(obs_t)
        feat = self.world_model.get_feature(state)
        q_values = self.q_network(feat).q_values
        q_selected = q_values.gather(1, actions_t.unsqueeze(1)).squeeze(1)
        
        loss = F.mse_loss(q_selected, target)
        
        self.q_optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 10.0)
        self.q_optimizer.step()
        
        # å®šæœŸæ›´æ–°target network
        if self.step_count % cfg.target_update_steps == 0:
            self.target_q_network.load_state_dict(self.q_network.state_dict())
    
    def on_episode_end(self):
        """Episodeç»“æŸæ—¶è°ƒç”¨"""
        self.episode_count += 1
        
        # â­ é€šçŸ¥adaptive controller
        self.adaptive_controller.on_episode_end()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        stats = self.adaptive_controller.get_stats()
        logger.info(f"Agent {self.agent_id} Episode {stats['episode']}:")
        logger.info(f"  Horizon: {stats['horizon']}")
        logger.info(f"  Real batch: {stats['real_batch']} (å›ºå®š)")
        logger.info(f"  Synthetic batch: {stats['synthetic_batch']}")
        logger.info(f"  Total batch: {stats['total_batch']}")
        logger.info(f"  Avg WM quality: {stats['avg_wm_quality']:.3f}")
    
    def _encode_obs(self, obs):
        """Encode observation to latent state"""
        return self.world_model.encode(obs)
    
    def _train_world_model(self):
        """è®­ç»ƒWorld Model - ä½¿ç”¨åºåˆ—é‡‡æ ·"""
        if len(self.replay_buffer) < cfg.get('seq_length', 50):
            return
        
        # Sample sequence
        seq_data = self.replay_buffer.sample_sequence(cfg.get('seq_length', 50))
        
        obs = torch.FloatTensor(seq_data['obs']).to(self.device)
        actions = torch.FloatTensor(seq_data['actions']).to(self.device)
        rewards = torch.FloatTensor(seq_data['rewards']).unsqueeze(-1).to(self.device)
        
        # Forward
        recon_obs, pred_rewards, kl_loss = self.world_model(obs, actions)
        
        # Losses
        recon_loss = F.mse_loss(recon_obs, obs)
        reward_loss = F.mse_loss(pred_rewards, rewards)
        
        # GSP loss
        gsp_loss = torch.tensor(0.0).to(self.device)
        if self.coordinator:
            state = self.world_model.encode(obs[:, -1:])
            G_hat = self.world_model.predict_global(state)
            
            G_consensus = self.coordinator.get_latest_consensus()
            if G_consensus is not None:
                G_consensus_t = torch.FloatTensor(G_consensus).unsqueeze(0).to(self.device)
                gsp_loss = F.mse_loss(G_hat, G_consensus_t)
        
        # Total loss
        total_loss = (recon_loss + 
                     reward_loss + 
                     cfg.get('beta_kl', 1.0) * kl_loss +
                     cfg.get('alpha_contrastive', 0.1) * gsp_loss)
        
        # Backward
        self.wm_optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.world_model.parameters(), 10.0)
        self.wm_optimizer.step()
    
    def get_quality_score(self):
        """è·å–WMè´¨é‡åˆ†æ•°"""
        if len(self.replay_buffer) < cfg.batch_size:
            return 0.5
        
        transitions = self.replay_buffer.sample_transitions(
            min(cfg.batch_size, len(self.replay_buffer))
        )
        obs = np.stack([t['obs'] for t in transitions])
        obs_t = torch.FloatTensor(obs).to(self.device)
        
        with torch.no_grad():
            state = self._encode_obs(obs_t)
            Q_score = self.world_model.get_quality_score(state)
            return Q_score.mean().item()
    
    def _upload_fl_params(self):
        """ä¸Šä¼ FLå‚æ•°"""
        if not self.coordinator:
            return
        
        params = {k: v.cpu().clone() for k, v in self.world_model.state_dict().items()}
        quality = self.get_quality_score()
        
        self.coordinator.receive_fl_update(self.agent_id, params, quality)
    
    def update_from_global(self, global_params):
        """ä»å…¨å±€å‚æ•°æ›´æ–°"""
        self.global_params = {k: v.clone().cpu() for k, v in global_params.items()}
        
        current_params = self.world_model.state_dict()
        updated_count = 0
        skipped_count = 0
        
        for key, global_param in global_params.items():
            if key in current_params:
                if current_params[key].shape == global_param.shape:
                    current_params[key] = global_param
                    updated_count += 1
                else:
                    skipped_count += 1
        
        self.world_model.load_state_dict(current_params)
        
        logger.info(f"Agent {self.agent_id}: Updated {updated_count} params, "
                   f"kept {skipped_count} local")


# ==================== RESCO Adapter ====================

class MBIDQN_Dyna_FL_GSP_Fixed_V2_Adaptive(IndependentAgent):
    """
    RESCO Adapter with Adaptive Control
    """
    def __init__(self, obs_act):
        super().__init__(obs_act)
        
        logger.info("=" * 70)
        logger.info("ğŸš€ MBIDQN Dyna + FL + GSP (Adaptive V2)")
        logger.info("=" * 70)
        logger.info("Key features:")
        logger.info("1. âœ… Real dataä¿åº•æœºåˆ¶ (min 50%)")
        logger.info("2. âœ… Synthetic dataä½œä¸ºå¢é‡")
        logger.info("3. âœ… åŠ¨æ€horizonè°ƒæ•´ (1â†’5)")
        logger.info("4. âœ… Quality-based adaptive control")
        logger.info("=" * 70)
        
        # åˆ›å»ºå…¨å±€æ¨¡å‹å’Œcoordinator
        first_agent_id = list(obs_act.keys())[0]
        obs_space = obs_act[first_agent_id][0]
        act_space = obs_act[first_agent_id][1]
        
        if len(obs_space) == 1:
            obs_dim = obs_space[0]
        else:
            obs_dim = int(np.prod(obs_space))
        
        # [åˆ›å»ºglobal modelå’Œcoordinatorçš„ä»£ç ä¸åŸv2ç›¸åŒ]
        # ...
        
        # åˆ›å»ºadaptive agents
        for agent_id in obs_act:
            obs_space = obs_act[agent_id][0]
            act_space = obs_act[agent_id][1]
            
            agent = DynaMBAgent_Adaptive(
                agent_id,
                obs_space,
                act_space,
                coordinator=self.coordinator
            )
            self.agents[agent_id] = agent
            self.coordinator.register_agent(agent_id)
        
        logger.info(f"Initialized {len(self.agents)} adaptive agents")
    
    def observe(self, observations, rewards, dones, infos):
        """æ‰©å±•observe - å¤„ç†episodeç»“æŸ"""
        super().observe(observations, rewards, dones, infos)
        
        # FLèšåˆ
        if self.coordinator.should_aggregate_fl():
            global_params = self.coordinator.aggregate_fl_and_broadcast()
            if global_params:
                for agent in self.agents.values():
                    agent.update_from_global(global_params)
        
        # GSPå…±è¯†
        current_step = list(self.agents.values())[0].global_step
        if self.coordinator.should_compute_consensus(current_step):
            G_consensus = self.coordinator.compute_consensus(current_step)
        
        # â­ æ£€æŸ¥episodeç»“æŸ
        if any(dones.values()):
            for agent in self.agents.values():
                agent.on_episode_end()


# ==================== éœ€è¦åœ¨agent.yamlä¸­æ·»åŠ çš„é…ç½® ====================
"""
MBIDQN_Dyna_FL_GSP_Fixed_V2_Adaptive:
  module: action_value.mbidqn_dyna_fl_gsp_fixed_v2_adaptive
  state: drq
  reward: wait
  
  # Learning Rates
  learning_rate: 1e-3
  lr_world_model: 5e-4
  
  # FLé…ç½®
  fl_interval: 100
  aggregation_method: quality_weighted
  alpha_fedprox: 0.01
  
  # GSPé…ç½®
  global_dim: 64
  alpha_contrastive: 0.1
  contrastive_temperature: 0.1
  gsp_sync_threshold: 0.8
  
  # RSSMé…ç½®
  seq_length: 50
  model_train_freq: 1
  
  # â­ Adaptive Controlé…ç½® - NEW
  min_real_ratio: 0.5              # Real dataæœ€å°æ¯”ä¾‹ï¼ˆä¿åº•ï¼‰
  max_synthetic_ratio: 0.7          # Synthetic dataæœ€å¤§æ¯”ä¾‹
  initial_horizon: 1                # åˆå§‹horizon
  imagination_horizon: 5            # æœ€å¤§horizon
  horizon_increase_freq: 10         # æ¯10ä¸ªepisodeså¢åŠ 1æ­¥horizon
  adaptive_factor: 0.5              # è‡ªé€‚åº”è°ƒæ•´å› å­
  
  # Dynaé…ç½®
  num_imagined_rollouts: 1
  imagination_freq: 5
  
  # Earlyé˜¶æ®µä¿æŠ¤
  model_warmup_steps: 1000
  min_wm_quality: 0.3
  
  # Q-Networké…ç½®
  batch_size: 32
  discount: 0.99
  target_update_steps: 500
  number_of_layers: 3
  number_of_units: 128
  
  # Exploration
  epsilon_begin: 1.0
  epsilon_end: 0.1
  epsilon_decay_period: 100000
  
  # Buffer
  buffer_size: 50000
  
  # Optimizer
  rmsprop_decay: 0.95
  rmsprop_epsilon: 0.00001
  rmsprop_momentum: 0.0
"""
