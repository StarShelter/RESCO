# The agent.yaml file contains the configuration of the agents used in the benchmark.
# The agents are defined by their name and the parameters that are used to configure them.

FIXED:
  module: static.fixed
  state: wave
  reward: wait  # Rewards are irrelevant for static agents
  action_set: Phase

STOCHASTIC:
  module: static.stochastic
  state: mplight
  reward: wait

MAXWAVE:
  module: static.maxwave
  state: wave
  reward: wait

MAXPRESSURE:
  module: static.maxpressure
  state: mplight
  reward: wait

RLCD:
  module: action_value.rlcd
  state: rlcd
  reward: rlcd
  discount: 0.99
  epsilon: 0.05
  alpha: 0.1
  action_set: PlanPick
  priority_offset: 2
  plan_steps: 24
  regular_limit: 10
  full_limit: 20
  M: 360
  omega: 0.5
  rho: 0.1
  min_E: -10

IMultiDQN:
  module: action_value.multiheaded_dqn
  state: drq
  reward: wait
  learning_rate: 1e-3
  batch_size: 32
  discount: 0.99
  epsilon_begin: 0.0
  epsilon_end: 0.0
  target_update_steps: 500
  buffer_size: 20000
  buffer_type: uniform
  epsilon_decay_period: 0.8 # unused
  number_of_layers: 3
  number_of_units: 64
  # Start of multidqn params
  phase_period: 48
  expert: maxpressure
  criteria: std
  global_buffers: False
  fixed_buffer: False
  init_time: 20
  sample_time: 5
  model_buffer_size: 86400
  deviations: 1
  p_value: 0.05
  fallback: 20
  intervals: [50, 100, 200, 300, 400, 500, 600, 700, 800, 1000, 1200, 1400, 1600]
  # SWOKS
  num_tasks: 8
  stablephase: 2160
  h_len: 240
  emd_limit: 125
  kolsmi_buffer: 42
  adj: 1.1
  alpha: 0.001


ISAC:
  module: policy.coom_sac
  state: drq
  reward: wait
  learning_rate: 9e-4 # Tuned on 2023-01-24 (randomly selected) 18:00 saltlake1B_state via CMA-ES for 1000 trials
  replay_size: 20000
  discount: 0.99
  number_of_layers: 3
  number_of_units: 64
  cl_method: null # one of [null, agem, clonex, ewc, l2, mas, owl, packnet, vcl]


SWOKS:
  module: policy.SWOKS.swoks
  state: drq
  reward: wait
  learning_rate: 0.007
  discount: 0.99
  flat_state: False
  rollout_length: 128
  num_mini_batches: 32
  ppo_ratio_clip: 0.1
  entropy_weight: 0.01
  gradient_clip: 5
  num_tasks: 8
  stablephase: 2160
  h_len: 240
  emd_limit: 125
  kolsmi_buffer: 42
  adj: 1.1
  alpha: 0.001
  rollback: 50

CoSLight:
  module: policy.CoSLight.coslight
  state: coslight
  reward: coslight
  discount: 0.99
  use_gae: True
  gae_lambda: 0.95
  use_popart: False
  use_valuenorm: True
  use_proper_time_limits: False
  recurrent_N: 1
  hidden_size: 64
  use_kl: True
  use_trans_hidden: True
  trans_hidden: 64
  trans_layers: 2
  trans_heads: 8
  hidden_layer_size: 12
  use_K: 1
  lr: 0.0005
  critic_lr: 5e-05
  opti_eps: 1e-05
  weight_decay: 0
  gain: 0.01
  use_orthogonal: True
  use_policy_active_masks: True
  use_naive_recurrent_policy: False
  use_recurrent_policy: True
  use_ReLU: False
  use_feature_normalization: True
  layer_N: 1
  clip_param: 0.2
  ppo_epoch: 10
  num_mini_batch: 4
  data_chunk_length: 10
  value_loss_coef: 1
  entropy_coef: 0.01
  max_grad_norm: 10
  huber_delta: 10.0
  use_max_grad_norm: True
  use_clipped_value_loss: True
  use_huber_loss: True
  use_value_active_masks: True
  use_3cons: True
  use_sym_loss: True
  use_pressure: False
  epsilon: 1.0
  state_key: ['current_phase', 'car_num', 'queue_length', 'occupancy', 'flow', 'stop_car_num', 'pressure']
  demand_shape: 7


IDQN:
  module: action_value.pfrl_dqn
  state: drq
  reward: wait
  learning_rate: 1e-3
  batch_size: 32
  discount: 0.99
  epsilon_begin: 1.0
  epsilon_end: 0.0
  target_update_steps: 500
  buffer_size: 20000
  buffer_type: uniform
  epsilon_decay_period: 0.8 # 80% of the total number of episodes
  number_of_layers: 3
  number_of_units: 64
  # Following are parameters only when using state_builder or reward_builder
  vehicles_detailed: 0
  state_builder: [
      "lane_aggregate", "direction_aggregate", "arrived", "departed",
      "phase_timers", "approaching", "queued",
      "effective_running", "average_wait", "average_delay", "average_speed",
      "average_acceleration", "average_deceleration", "max_wait", "max_delay",
      "max_speed", "max_acceleration", "max_deceleration", "min_wait",
      "min_delay", "min_speed", "min_acceleration", "min_deceleration", "wait",
      "delay", "speed", "acceleration", "deceleration", "time", "pressure",
      "downstream_arrived", "downstream_departed", "downstream_phase_timers",
      "downstream_pressure", "downstream_approaching",
      "downstream_queued", "downstream_effective_running",
      "downstream_average_wait", "downstream_average_delay",
      "downstream_average_speed", "downstream_average_acceleration",
      "downstream_average_deceleration", "downstream_max_wait",
      "downstream_max_delay", "downstream_max_speed", "downstream_max_acceleration",
      "downstream_max_deceleration", "downstream_min_wait", "downstream_min_delay",
      "downstream_min_speed", "downstream_min_acceleration",
      "downstream_min_deceleration", "downstream_wait", "downstream_delay",
      "downstream_speed", "downstream_acceleration", "downstream_deceleration",
  ]
  reward_builder: ["average_wait"]
  reward_aggregation: "sum"
  reward_scale: -1


MPLight:
  module: action_value.mplight
  state: mplight
  reward: pressure
  learning_rate: 1e-3
  demand_shape: 1 # Size of per-direction demand input, 1 for pressure per direction mplight state
  batch_size: 32
  discount: 0.99
  epsilon_begin: 1.0
  epsilon_end: 0.0
  target_update_steps: 500
  buffer_size: 20000
  buffer_type: uniform
  epsilon_decay_period: 0.8


AdvancedMPLight:
  module: action_value.advanced_mplight
  state: advanced_mplight
  reward: pressure
  learning_rate: 1e-3
  demand_shape: 1
  batch_size: 32
  discount: 0.99
  epsilon_begin: 1.0
  epsilon_end: 0.0
  target_update_steps: 500
  buffer_size: 20000
  buffer_type: uniform
  epsilon_decay_period: 0.8


IPPO:
  module: policy.pfrl_ppo
  state: drq
  reward: wait
  learning_rate: 2.5e-5
  number_of_layers: 3
  number_of_units: 64
  adam_epsilon: 1e-5
  clip_eps: 0.1
  update_interval: 1024
  batch_size: 256
  epochs: 4
  entropy_coef: 0.001
  max_grad_norm: 0.5
  standardize_advantages: True


FMA2C:
  module: policy.fma2c
  state: fma2c
  reward: fma2c
  learning_rate: 1e-3
  management_acts: 4
  rmsp_alpha: 0.99
  rmsp_epsilon: 1e-5
  max_grad_norm: 40
  gamma: 0.96
  lr_init: 2.5e-4
  lr_decay: constant
  entropy_coef_init: 0.001
  entropy_coef_min: 0.001
  entropy_decay: constant
  entropy_ratio: 0.5
  value_coef: 0.5
  num_lstm: 64
  num_fw: 128
  num_ft: 32
  num_fp: 64
  batch_size: 120
  reward_norm: 2000.0
  reward_clip: 2.0
  coef: 0.4
  coop_gamma: 0.9
  clip_wave: 4.0
  clip_wait: 4.0
  norm_wave: 5.0
  norm_wait: 100.0
  alpha: 0.75

# ==================== Adaptive Model-Based IDQN Configuration ====================
#
# 核心改进：动态控制real/synthetic比例和horizon
#
# 关键概念：
# 1. Real data保底：min_real_ratio确保真实数据最小比例（如50%）
# 2. Synthetic data作为增量：根据WM质量动态增加
# 3. Horizon动态增长：从小horizon逐步增加，避免early阶段的compounding error
# 4. Quality监控：根据WM质量自适应调整策略
#
# ====================================================================================

# ==================== Dyna版本：Adaptive Synthetic Batch Control ====================
# 
# 核心策略：
# - Real data batch固定（如32），确保训练信号强度
# - Synthetic data batch动态增加（0到max_synthetic_batch），作为增量bonus
# - 总batch size = 32 (real) + X (synthetic)，X根据WM质量动态调整
#
MBIDQN_Dyna_FL_GSP_Fixed_V2_Adaptive:
  module: action_value.mbidqn_dyna_fl_gsp_fixed_v2_adaptive
  state: drq
  reward: wait
  
  # ==================== Learning Rates ====================
  learning_rate: 1e-3              # Q-Network学习率
  lr_world_model: 5e-4             # World Model学习率（Dyna可以稍大）
  
  # ==================== FL配置 ====================
  fl_interval: 100                 # FL聚合间隔（每100步）
  aggregation_method: quality_weighted  # 质量加权聚合
  alpha_fedprox: 0.01              # FedProx正则化系数
  
  # ==================== GSP配置 ====================
  global_dim: 64                   # 全局状态预测维度
  alpha_contrastive: 0.1           # GSP对比损失权重
  contrastive_temperature: 0.1     # InfoNCE温度
  gsp_sync_threshold: 0.8          # GSP同步阈值
  
  # ==================== RSSM配置 ====================
  seq_length: 50                   # 序列长度（必须连续采样！）
  model_train_freq: 1              # WM训练频率
  
  # ==================== ⭐ Adaptive Control配置 - CORE ⭐ ====================
  # 
  # Real batch固定：
  batch_size: 32                   # Real data每次固定32个样本（不变！）
  
  # Synthetic batch动态增量：
  max_synthetic_batch: 64          # Synthetic data最大batch size（增量上限）
                                   # 例如：quality=0 → synthetic=0, quality=1.0 → synthetic=64
  adaptive_factor: 1.0             # 自适应调整因子（控制增长速度）
                                   # synthetic = (quality - threshold) / (1 - threshold) * factor * max
  
  # Horizon动态增长：
  initial_horizon: 1               # 初始imagination horizon（从1步开始）
  imagination_horizon: 5           # 最大imagination horizon（逐步增长到5步）
  horizon_increase_freq: 10        # 每10个episodes增加1步horizon
  
  # ==================== Dyna配置 ====================
  num_imagined_rollouts: 1         # 每次生成的imagined轨迹数量
  imagination_freq: 5              # 每5步生成一次imagined data
  
  # ==================== Early阶段保护 ====================
  model_warmup_steps: 1000         # 前1000步不使用imagination
  min_wm_quality: 0.3              # WM质量阈值（低于此值不用synthetic data）
  
  # ==================== Q-Network配置 ====================
  discount: 0.99
  target_update_steps: 500
  number_of_layers: 3
  number_of_units: 128
  
  # ==================== Exploration ====================
  epsilon_begin: 1.0
  epsilon_end: 0.1
  # epsilon_decay_period: 100000
  epsilon_decay_period: 500
  
  # ==================== Buffer ====================
  buffer_size: 50000
  
  # ==================== Optimizer ====================
  rmsprop_decay: 0.95
  rmsprop_epsilon: 0.00001
  rmsprop_momentum: 0.0


# ==================== True版本：Adaptive Imagination Horizon ====================
# 
# 策略：
# - 不需要real/synthetic混合（Q在imagination中训练）
# - Horizon从initial_horizon逐步增长
# - 根据WM质量动态调整horizon（quality_scaling=True）
#
MBIDQN_True_FL_GSP_Fixed_V2_Adaptive:
  module: action_value.mbidqn_true_fl_gsp_fixed_v2_adaptive
  state: drq
  reward: wait
  
  # ==================== Learning Rates ====================
  learning_rate: 5e-4              # Q-Network学习率（True版本稍小）
  lr_world_model: 1e-4             # World Model学习率（更保守）
  
  # ==================== FL配置 ====================
  fl_interval: 100
  aggregation_method: quality_weighted
  alpha_fedprox: 0.01
  
  # ==================== GSP配置 ====================
  global_dim: 64
  alpha_contrastive: 0.1
  contrastive_temperature: 0.1
  gsp_sync_threshold: 0.8
  
  # ==================== RSSM配置 ====================
  seq_length: 50
  model_train_freq: 1
  
  # ==================== ⭐ Adaptive Imagination配置 - CORE ⭐ ====================
  # 
  # Horizon动态增长：
  initial_horizon: 1               # 初始horizon（从1步开始，避免early compounding error）
  imagination_horizon: 5           # 最大horizon（逐步增长到5步）
  horizon_increase_freq: 10        # 每10个episodes增加1步
  quality_scaling: True            # 根据WM质量动态调整horizon
                                   # quality < 0.5 → horizon - 2
                                   # quality > 0.8 → horizon + 1
  
  # ==================== Early阶段保护 ====================
  model_warmup_steps: 1000         # 前1000步不在imagination中训练Q
  min_wm_quality: 0.3              # WM质量阈值
  
  # ==================== Q-Network配置 ====================
  batch_size: 32
  discount: 0.99
  target_update_steps: 500
  number_of_layers: 3
  number_of_units: 128
  
  # ==================== Exploration ====================
  epsilon_begin: 1.0
  epsilon_end: 0.1
  epsilon_decay_period: 100000
  
  # ==================== Buffer ====================
  buffer_size: 50000
  
  # ==================== Optimizer ====================
  rmsprop_decay: 0.95
  rmsprop_epsilon: 0.00001
  rmsprop_momentum: 0.0


# ==================== 配置说明 ====================
#
# 1. Dyna vs True的区别：
#    - Dyna: 控制real/synthetic比例（因为生成数据到buffer）
#    - True: 只控制imagination horizon（Q在imagination中训练）
#
# 2. 核心超参数调优建议：
#    
#    min_real_ratio (Dyna):
#    - 越大越保守（更多real data）
#    - 推荐：0.5-0.7
#    - 如果WM质量不稳定，增大此值
#    
#    initial_horizon:
#    - 越小越保守（减少early compounding error）
#    - 推荐：1-2
#    - Early阶段从小horizon开始很重要！
#    
#    imagination_horizon:
#    - 最终的最大horizon
#    - 推荐：3-7
#    - 取决于任务复杂度和WM质量
#    
#    horizon_increase_freq:
#    - 越大越保守（horizon增长越慢）
#    - 推荐：10-20
#    - 让WM有充分时间提高质量
#    
#    adaptive_factor (Dyna):
#    - 控制质量对比例的影响程度
#    - 推荐：0.3-0.7
#    - 越大，质量好时synthetic比例越高
#    
#    quality_scaling (True):
#    - True: 根据质量动态调整horizon
#    - False: 只按episode增长，不看质量
#    - 推荐：True（更安全）
#
# 3. 典型训练过程：
#    
#    Episode 0-10:
#    - Warmup阶段，100% real data
#    - Horizon = 1
#    
#    Episode 10-20:
#    - WM质量提升
#    - Dyna: Real ratio 80% → 60%
#    - Horizon = 1 → 2
#    
#    Episode 20-50:
#    - WM质量稳定
#    - Dyna: Real ratio 60% → 50%
#    - Horizon = 2 → 5
#    
#    Episode 50+:
#    - 稳定阶段
#    - Dyna: Real ratio ~50-60%（根据质量波动）
#    - Horizon = 5（质量好时）或3-4（质量差时）
#
# 4. Debug建议：
#    - 观察log中的adaptive controller统计信息
#    - 监控WM质量、real_ratio、horizon的变化
#    - 如果performance不稳定，增大min_real_ratio或减小max_synthetic_ratio
#    - 如果convergence太慢，可以减小horizon_increase_freq
